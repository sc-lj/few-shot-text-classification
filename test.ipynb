{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4dcc432a33e63dbb610b875602eb044c0e155c5337a5a68ab0597c12bbef36c9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "from utils import load_pkl\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import configparser\n",
    "from model import Encoder,FewShotInduction\n",
    "class classifyModel(Encoder):\n",
    "    def __init__(self,num_classes, num_support_per_class,\n",
    "                 vocab_size, embed_size, hidden_size,\n",
    "                 output_dim, weights):\n",
    "        super(classifyModel,self).__init__(num_classes, num_support_per_class,\n",
    "                 vocab_size, embed_size, hidden_size,\n",
    "                 output_dim, weights)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.logit = nn.Linear(hidden_size,2)\n",
    "\n",
    "    def forward(self,input_id):\n",
    "        output,_ = super().forward(input_id)\n",
    "        linear1 = self.linear(output)\n",
    "        logit = self.logit(linear1)\n",
    "        return logit\n",
    "word2index,weights = load_pkl(\"./word2index_weight.pkl\")\n",
    "model = classifyModel(num_classes=1,num_support_per_class=20,\n",
    "                                        vocab_size=5661,embed_size=300,\n",
    "                                        hidden_size=128,\n",
    "                                        output_dim=64,\n",
    "                                        weights=weights\n",
    "                                        )\n",
    "model_dict = model.state_dict()\n",
    "save_model = torch.load('ckpt.pth')\n",
    "state_dict = {compiles.match(k).group(2):v for k,v in save_model.items() if k.startswith(\"encoder\")}\n",
    "model_dict.update(state_dict)\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0881,  0.0178,  0.0313,  ..., -0.1605,  0.0250,  0.0244],\n",
       "        [ 0.0185,  0.1489, -0.0460,  ...,  0.0887, -0.0362, -0.0914],\n",
       "        [-0.0649, -0.0173,  0.0188,  ..., -0.0674,  0.0294, -0.0591],\n",
       "        ...,\n",
       "        [-0.0158, -0.0164, -0.1370,  ...,  0.0173,  0.0328,  0.0109],\n",
       "        [ 0.0281, -0.0385, -0.0009,  ..., -0.0692, -0.0022,  0.0479],\n",
       "        [ 0.0087,  0.0091, -0.1739,  ...,  0.0828, -0.1112,  0.0797]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "dict(model.named_parameters())[\"bilstm.weight_ih_l0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0564, -0.0626, -0.0798,  ..., -0.0798,  0.0707, -0.0712],\n",
       "        [ 0.0285,  0.0107, -0.0256,  ...,  0.0845,  0.0790, -0.0172],\n",
       "        [ 0.0546,  0.0607,  0.0286,  ...,  0.0061,  0.0074,  0.0664],\n",
       "        ...,\n",
       "        [-0.0789, -0.0381, -0.0407,  ..., -0.0166, -0.0845, -0.0464],\n",
       "        [ 0.0330, -0.0839,  0.0523,  ..., -0.0810,  0.0478, -0.0605],\n",
       "        [ 0.0656, -0.0618,  0.0411,  ..., -0.0355, -0.0806,  0.0268]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "dict(model.named_parameters())[\"bilstm.weight_ih_l0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'fc2.weight'"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "name = 'encoder.fc2.weight'\n",
    "import re\n",
    "compiles =re.compile(\"(encoder\\.)(.*)\",re.S)\n",
    "compiles.match(name).group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['encoder.embedding.weight',\n",
       " 'encoder.bilstm.weight_ih_l0',\n",
       " 'encoder.bilstm.weight_hh_l0',\n",
       " 'encoder.bilstm.bias_ih_l0',\n",
       " 'encoder.bilstm.bias_hh_l0',\n",
       " 'encoder.bilstm.weight_ih_l0_reverse',\n",
       " 'encoder.bilstm.weight_hh_l0_reverse',\n",
       " 'encoder.bilstm.bias_ih_l0_reverse',\n",
       " 'encoder.bilstm.bias_hh_l0_reverse',\n",
       " 'encoder.fc1.weight',\n",
       " 'encoder.fc1.bias',\n",
       " 'encoder.fc2.weight',\n",
       " 'encoder.fc2.bias',\n",
       " 'induction.W.weight',\n",
       " 'induction.W.bias',\n",
       " 'relation.M',\n",
       " 'relation.W.weight',\n",
       " 'relation.W.bias']"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "[k for k,v in save_model.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['embedding.weight',\n",
       " 'bilstm.weight_ih_l0',\n",
       " 'bilstm.weight_hh_l0',\n",
       " 'bilstm.bias_ih_l0',\n",
       " 'bilstm.bias_hh_l0',\n",
       " 'bilstm.weight_ih_l0_reverse',\n",
       " 'bilstm.weight_hh_l0_reverse',\n",
       " 'bilstm.bias_ih_l0_reverse',\n",
       " 'bilstm.bias_hh_l0_reverse',\n",
       " 'fc1.weight',\n",
       " 'fc1.bias',\n",
       " 'fc2.weight',\n",
       " 'fc2.bias']"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "[ compiles.match(k).group(2)\n",
    " for k,v in save_model.items() if k.startswith(\"encoder\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['embedding.weight',\n",
       " 'bilstm.weight_ih_l0',\n",
       " 'bilstm.weight_hh_l0',\n",
       " 'bilstm.bias_ih_l0',\n",
       " 'bilstm.bias_hh_l0',\n",
       " 'bilstm.weight_ih_l0_reverse',\n",
       " 'bilstm.weight_hh_l0_reverse',\n",
       " 'bilstm.bias_ih_l0_reverse',\n",
       " 'bilstm.bias_hh_l0_reverse',\n",
       " 'fc1.weight',\n",
       " 'fc1.bias',\n",
       " 'fc2.weight',\n",
       " 'fc2.bias',\n",
       " 'linear.weight',\n",
       " 'linear.bias',\n",
       " 'logit.weight',\n",
       " 'logit.bias']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "[k for k,v in model_dict.items()]"
   ]
  }
 ]
}